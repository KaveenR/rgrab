#!/usr/bin/env python2
# -*- coding: utf-8 -*-

import urllib,re,json,os,sys

null = ""
true = True
false = False

import argparse

def download_image(url,sub):
	name  = url.split('/')[-1]
	nameF = sub+"_"+name

	if os.path.isfile(nameF):
		print("Already Downloaded Skipping")
	else:
		urllib.urlretrieve(url,sub+"_"+name)
		print("100% " + name)


def download(name,contin):
    try:
        website = urllib.urlopen("http://www.reddit.com/r/"+name+"/.json?count="+str(contin))
    	jsonb = json.loads(website.read())
    except:
        print("Data retrival failed")
        sys.exit()

    links = []

    for i in jsonb['data']['children']:
		links.append(i['data']['url'])

    for i in links:
	       if i[(len(i)-4):(len(i))].upper() in ['.JPG','.PNG','.GIF']:
			            download_image(i,name)

def initParser():
    parser = argparse.ArgumentParser(description="Simple Reddit image downloader")

    parser.add_argument('-n','--name', action='store', dest='subreddit',
                        help='sub-reddit name', required=True)
    parser.add_argument('-c','--continue', action='store', dest='count',
                        help='continue count', required=False, default= 0)
    results = parser.parse_args()
    download(results.subreddit,results.count)

if __name__ == "__main__":
    initParser()
